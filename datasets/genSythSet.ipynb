{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# mass includes\n",
    "import os, sys, warnings\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import rawpy as rp\n",
    "import scipy.stats as stat\n",
    "from random import choices\n",
    "from skimage.transform.pyramids import pyramid_reduce\n",
    "from numpy.random import uniform, normal, poisson\n",
    "from scipy.stats import tukeylambda\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# add paths for all sub-folders\n",
    "paths = [root for root, _, _ in os.walk('..') if 'evals' not in root]\n",
    "for item in paths:\n",
    "    sys.path.append(item)\n",
    "\n",
    "from ipynb.fs.full.network import *\n",
    "from ipynb.fs.full.config import *\n",
    "from ipynb.fs.full.util import unifyBayerPtn, cam2sRGB, downsize\n",
    "from ipynb.fs.full.ISPColor import extMetadata, cam2xyzWP, cam2xyzD50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/lab/Documents/SSD/PlieCNR/RAISE'\n",
    "model_path = '/home/lab/Documents/SSD/PlieCNR/cameras'\n",
    "cam_model = 'S20'\n",
    "exp_range = (1, 16)  # exposure range\n",
    "highlight = 0.95  # preserve highlight\n",
    "num_train = 8000  # number of training samples\n",
    "num_val = 80  # number of validation samples\n",
    "min_px = 320  # minimal output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# for debugging only\n",
    "%pdb off\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# choose CPU because of parallel computing\n",
    "device = t.device('cpu')\n",
    "\n",
    "# find all samples\n",
    "file_list = [\n",
    "    file for file in os.listdir(data_path) if '.dng' in file or '.DNG' in file\n",
    "]\n",
    "file_list.sort()\n",
    "print('Found %d samples.' % len(file_list))\n",
    "\n",
    "# define model\n",
    "opt = Config('unpaired')\n",
    "net_E = Enhancer()\n",
    "net_E.load('../saves')\n",
    "net_E.to(device).eval()\n",
    "for param in net_E.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     20
    ]
   },
   "outputs": [],
   "source": [
    "# generate a list random noise parameters\n",
    "def genParams(num_samples):\n",
    "    # random sample list\n",
    "    sample_list = choices(file_list, k=num_samples)\n",
    "\n",
    "    k_list = uniform(low=noise_model['k'][0],\n",
    "                     high=noise_model['k'][1],\n",
    "                     size=num_samples)\n",
    "    sig_r_list = noise_model['sig_r'][0] * k_list + noise_model['sig_r'][\n",
    "        1] + normal(loc=0.0, scale=noise_model['sig_r'][2], size=num_samples)\n",
    "    sig_tl_list = noise_model['sig_tl'][0] * k_list + noise_model['sig_tl'][\n",
    "        1] + normal(loc=0.0, scale=noise_model['sig_tl'][2], size=num_samples)\n",
    "\n",
    "    params = zip(range(num_samples), sample_list, k_list, sig_r_list,\n",
    "                 sig_tl_list)\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# generate noise\n",
    "def genSample(params):\n",
    "    index = params[0]\n",
    "    file = params[1]\n",
    "    log_k = params[2]\n",
    "    log_sig_r = params[3]\n",
    "    log_sig_tl = params[4]\n",
    "\n",
    "    # load a new file\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    with rp.imread(file_path) as raw_obj:\n",
    "        raw_bayer = raw_obj.raw_image_visible.copy()\n",
    "        raw_mask = raw_obj.raw_colors_visible\n",
    "        blk_level = raw_obj.black_level_per_channel\n",
    "        sat_level = raw_obj.white_level\n",
    "        raw_type = raw_obj.raw_pattern\n",
    "\n",
    "    # normalize to 0-1\n",
    "    raw_bayer = raw_bayer.astype(np.float64)\n",
    "    raw_bayer[raw_mask == 0] = raw_bayer[raw_mask == 0] - blk_level[0]\n",
    "    raw_bayer[raw_mask == 1] = raw_bayer[raw_mask == 1] - blk_level[1]\n",
    "    raw_bayer[raw_mask == 2] = raw_bayer[raw_mask == 2] - blk_level[2]\n",
    "    raw_bayer[raw_mask == 3] = raw_bayer[raw_mask == 3] - blk_level[3]\n",
    "    raw_bayer = raw_bayer / (sat_level - max(blk_level))\n",
    "    raw_bayer = np.clip(raw_bayer, 0.0, 1.0)\n",
    "\n",
    "    # random exposure shift with highlight preservation\n",
    "    exposure = uniform(exp_range[0], exp_range[1])\n",
    "    exponent = math.log(highlight / exposure, highlight)\n",
    "    raw_bayer = np.where(raw_bayer < highlight, raw_bayer / exposure,\n",
    "                         raw_bayer**exponent)\n",
    "\n",
    "    # Bayer pattern unification\n",
    "    raw_bayer = unifyBayerPtn(raw_bayer, raw_type)\n",
    "\n",
    "    # compute proper scale\n",
    "    hei, wid = raw_bayer.shape\n",
    "    scale = math.ceil(min(hei / 2, wid / 2)) / float(min_px)\n",
    "\n",
    "    # downsize and pack to 4-channel raw\n",
    "    r_chn = pyramid_reduce(raw_bayer[0::2, 0::2], downscale=scale)\n",
    "    g1_chn = pyramid_reduce(raw_bayer[0::2, 1::2], downscale=scale)\n",
    "    g2_chn = pyramid_reduce(raw_bayer[1::2, 0::2], downscale=scale)\n",
    "    b_chn = pyramid_reduce(raw_bayer[1::2, 1::2], downscale=scale)\n",
    "    clean_raw = np.stack([r_chn, g1_chn, g2_chn, b_chn], axis=-1)\n",
    "    clean_raw = np.clip(clean_raw, 0.0, 1.0)\n",
    "\n",
    "    # add random shot noise\n",
    "    shot_anlg = clean_raw / np.exp(log_k)\n",
    "    noisy_raw = np.exp(log_k) * poisson(lam=shot_anlg)\n",
    "\n",
    "    # add random row noise\n",
    "    row_noise = normal(scale=np.exp(log_sig_r),\n",
    "                       size=(clean_raw.shape[0], 1, 2))\n",
    "    noisy_raw[:, :, 0] = noisy_raw[:, :, 0] + row_noise[:, :, 0]\n",
    "    noisy_raw[:, :, 1] = noisy_raw[:, :, 1] + row_noise[:, :, 0]\n",
    "    noisy_raw[:, :, 2] = noisy_raw[:, :, 2] + row_noise[:, :, 1]\n",
    "    noisy_raw[:, :, 3] = noisy_raw[:, :, 3] + row_noise[:, :, 1]\n",
    "\n",
    "    # add random read noise\n",
    "    read_noise = tukeylambda.rvs(noise_model['lam_tl'],\n",
    "                                 scale=np.exp(log_sig_tl),\n",
    "                                 size=clean_raw.shape)\n",
    "    noisy_raw = noisy_raw + read_noise\n",
    "\n",
    "    # add random quantization noise\n",
    "    qtz_noise = uniform(low=-noise_model['hf_step'],\n",
    "                        high=noise_model['hf_step'],\n",
    "                        size=clean_raw.shape)\n",
    "    noisy_raw = noisy_raw + qtz_noise\n",
    "\n",
    "    # clip to 0-1\n",
    "    noisy_raw = np.clip(noisy_raw, 0.0, 1.0)\n",
    "\n",
    "    # compute noise map\n",
    "    noise_map = np.exp(log_k) * noisy_raw + np.exp(log_sig_r)**2 + np.exp(\n",
    "        log_sig_tl)**2 + noise_model['hf_step']**2 / 3\n",
    "\n",
    "    # compute camera to xyz color space matrix\n",
    "    metadata = extMetadata(file_path)\n",
    "    wp_xyz, interp_w = cam2xyzWP(metadata)\n",
    "    cam2xyz = cam2xyzD50(metadata, wp_xyz, interp_w)\n",
    "\n",
    "    # downsize\n",
    "    raw_t = t.tensor(clean_raw.astype(np.float32)).to(device)\n",
    "    cam2xyz_t = t.tensor(cam2xyz.astype(np.float32)).to(device)\n",
    "    raw_t = raw_t.permute(2, 0, 1).unsqueeze(0)\n",
    "    cam2xyz_t = cam2xyz_t.unsqueeze(0)\n",
    "\n",
    "    srgb_img_t = cam2sRGB(raw_t, cam2xyz_t)\n",
    "    down_img_t = downsize(srgb_img_t)\n",
    "\n",
    "    # inference\n",
    "    with t.no_grad():\n",
    "        ilm_coes_t, _ = net_E(down_img_t, srgb_img_t)\n",
    "        ilm_coes = ilm_coes_t.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # save to file\n",
    "    save_dict = {}\n",
    "    save_dict['noisy_raw'] = noisy_raw.astype(np.float32)\n",
    "    save_dict['clean_raw'] = clean_raw.astype(np.float32)\n",
    "    save_dict['variance'] = noise_map.astype(np.float32)\n",
    "    save_dict['ilm_coes'] = ilm_coes.astype(np.float32)\n",
    "    save_dict['cam2xyz'] = cam2xyz.astype(np.float32)\n",
    "\n",
    "    save_file = 'sam_%05d.pkl' % index\n",
    "\n",
    "    return save_file, save_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load noise model\n",
    "with open(os.path.join(model_path, cam_model, 'noiseModel', 'model_stats.pkl'),\n",
    "          'rb') as pkl:\n",
    "    global noise_model\n",
    "    noise_model = pickle.load(pkl)\n",
    "\n",
    "# generate training set\n",
    "save_path = os.path.join(data_path, '../denoise', cam_model, 'train')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "params = genParams(num_train)\n",
    "for sample in tqdm(params, desc='train', total=num_train):\n",
    "    save_file, save_dict = genSample(sample)\n",
    "    with open(os.path.join(save_path, save_file), 'wb') as pkl:\n",
    "        pickle.dump(save_dict, pkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# generate validation set\n",
    "save_path = os.path.join(data_path, '../denoise', cam_model, 'val')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "params = genParams(num_val)\n",
    "for sample in tqdm(params, desc='val', total=num_val):\n",
    "    save_file, save_dict = genSample(sample)\n",
    "    with open(os.path.join(save_path, save_file), 'wb') as pkl:\n",
    "        pickle.dump(save_dict, pkl, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
