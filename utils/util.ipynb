{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mass includes\n",
    "import numpy as np\n",
    "import math as m\n",
    "import torch as t\n",
    "import torchvision as tv\n",
    "from numpy.random import randint, uniform\n",
    "from scipy import signal\n",
    "from colour_demosaicing import demosaicing_CFA_Bayer_DDFAPD as DDFAPD\n",
    "from torch.nn.functional import grid_sample, mse_loss, pad, conv2d, interpolate, avg_pool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayer pattern unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def unifyBayerPtn(in_cfa, cfa_type):\n",
    "    # check bayer type\n",
    "    color = {0: 'R', 1: 'G', 2: 'B', 3: 'G'}\n",
    "    cfa_type = cfa_type.reshape(-1)\n",
    "    cfa_type = color[cfa_type[0]] + color[cfa_type[1]] + color[\n",
    "        cfa_type[2]] + color[cfa_type[3]]\n",
    "\n",
    "    # GRBG to RGGB\n",
    "    if cfa_type == 'GRBG':\n",
    "        out_cfa = in_cfa[:, 1:-1].copy()\n",
    "\n",
    "    # GBRG to RGGB\n",
    "    elif cfa_type == 'GBRG':\n",
    "        out_cfa = in_cfa[1:-1, :].copy()\n",
    "\n",
    "    # BGGR to RGGB\n",
    "    elif cfa_type == 'BGGR':\n",
    "        out_cfa = in_cfa[1:-1, 1:-1].copy()\n",
    "\n",
    "    # native RGGB\n",
    "    else:\n",
    "        out_cfa = in_cfa.copy()\n",
    "\n",
    "    return out_cfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     29,
     44
    ]
   },
   "outputs": [],
   "source": [
    "# random cropping\n",
    "def batchCrop(in_imgs, crop_size, centred=False):\n",
    "    # generate random coordinates if necessary\n",
    "    if isinstance(in_imgs, list):\n",
    "        hei, wid, _ = in_imgs[0].shape\n",
    "    else:\n",
    "        hei, wid, _ = in_imgs.shape\n",
    "    min_dim = min(crop_size, hei - 1, wid - 1)\n",
    "    if centred == False:\n",
    "        crop_y = randint(hei - min_dim)\n",
    "        crop_x = randint(wid - min_dim)\n",
    "    else:\n",
    "        crop_y = round((hei - crop_size) / 2)\n",
    "        crop_x = round((wid - crop_size) / 2)\n",
    "\n",
    "    # crop all images\n",
    "    if isinstance(in_imgs, list):\n",
    "        out_imgs = []\n",
    "        for img in in_imgs:\n",
    "            out_imgs.append(img[crop_y:crop_y + crop_size,\n",
    "                                crop_x:crop_x + crop_size, :].copy())\n",
    "    else:\n",
    "        out_imgs = in_imgs[crop_y:crop_y + crop_size,\n",
    "                           crop_x:crop_x + crop_size, :].copy()\n",
    "\n",
    "    return out_imgs\n",
    "\n",
    "\n",
    "# random horizontal flipping\n",
    "def randHorFlip(in_imgs):\n",
    "    out_imgs = []\n",
    "    if uniform() > 0.5:\n",
    "        if isinstance(in_imgs, list):\n",
    "            for img in in_imgs:\n",
    "                out_imgs.append(np.fliplr(img).copy())\n",
    "        else:\n",
    "            out_imgs = np.fliplr(in_imgs).copy()\n",
    "    else:\n",
    "        out_imgs = in_imgs\n",
    "\n",
    "    return out_imgs\n",
    "\n",
    "\n",
    "# random vertical flipping\n",
    "def randVerFlip(in_imgs):\n",
    "    out_imgs = []\n",
    "    if uniform() > 0.5:\n",
    "        if isinstance(in_imgs, list):\n",
    "            for img in in_imgs:\n",
    "                out_imgs.append(np.flipud(img).copy())\n",
    "        else:\n",
    "            out_imgs = np.flipud(in_imgs).copy()\n",
    "\n",
    "    else:\n",
    "        out_imgs = in_imgs\n",
    "\n",
    "    return out_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image downsizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def downsize(in_img, out_size=256):\n",
    "    # compute sigma\n",
    "    _, chn, hei, wid = in_img.size()\n",
    "    ratio = hei / out_size\n",
    "    sigma = 2 * ratio / 6\n",
    "\n",
    "    # construct Gaussian kernel\n",
    "    padding = round(3 * sigma)\n",
    "    gauss_knl = t.arange(-padding, padding + 1, device=in_img.device)\n",
    "    gauss_knl = t.exp(-0.5 * gauss_knl**2 / sigma**2)\n",
    "    gauss_knl /= t.sum(gauss_knl[:])\n",
    "    gauss_knl = gauss_knl.repeat(chn, 1)\n",
    "\n",
    "    # add padding\n",
    "    out_img = pad(in_img, (padding, padding, padding, padding), mode='reflect')\n",
    "\n",
    "    # x direction 1D convolution\n",
    "    conv_kernel = gauss_knl.view(chn, 1, 1, -1)\n",
    "    out_img = conv2d(out_img, conv_kernel, groups=chn)\n",
    "\n",
    "    # y direction 1D convolution\n",
    "    conv_kernel = gauss_knl.view(chn, 1, -1, 1)\n",
    "    out_img = conv2d(out_img, conv_kernel, groups=chn)\n",
    "\n",
    "    # downsample\n",
    "    out_img = interpolate(out_img, size=(out_size, out_size), mode='nearest')\n",
    "\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color space transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     25
    ]
   },
   "outputs": [],
   "source": [
    "def cam2sRGB(in_img, cam2xyz):\n",
    "    xyz2srgb = cam2xyz.new_tensor([[3.1339, -1.6169, -0.4907],\n",
    "                                   [-0.9784, 1.9158, 0.0334],\n",
    "                                   [0.0720, -0.2290, 1.4057]])\n",
    "\n",
    "    bch, ch, hei, wid = in_img.size()\n",
    "    if ch == 4:\n",
    "        out_img = in_img.new_zeros(\n",
    "            (in_img.size(0), 3, in_img.size(2), in_img.size(3)))\n",
    "        out_img[:, 0, :, :] = in_img[:, 0, :, :].clone()\n",
    "        out_img[:, 1, :, :] = (in_img[:, 1, :, :] + in_img[:, 2, :, :]) / 2\n",
    "        out_img[:, 2, :, :] = in_img[:, 3, :, :].clone()\n",
    "    else:\n",
    "        out_img = in_img.clone()\n",
    "\n",
    "    out_img = out_img.view((bch, 3, -1))\n",
    "    out_img = t.matmul(cam2xyz, out_img)\n",
    "    out_img = t.clamp(out_img, 0.0, 1.0)\n",
    "    out_img = t.matmul(xyz2srgb, out_img)\n",
    "    out_img = t.clamp(out_img, 0.0, 1.0)\n",
    "    out_img = out_img.view((bch, 3, hei, wid))\n",
    "\n",
    "    return out_img\n",
    "\n",
    "\n",
    "def rgb2lumin(in_img, cam2xyz):\n",
    "    bch, ch, hei, wid = in_img.size()\n",
    "    if ch == 4:\n",
    "        lmn_coes = cam2xyz[:, 1].unsqueeze(1)\n",
    "        out_img = in_img.new_zeros(\n",
    "            (in_img.size(0), 3, in_img.size(2), in_img.size(3)))\n",
    "        out_img[:, 0, :, :] = in_img[:, 0, :, :].clone()\n",
    "        out_img[:, 1, :, :] = (in_img[:, 1, :, :] + in_img[:, 2, :, :]) / 2\n",
    "        out_img[:, 2, :, :] = in_img[:, 3, :, :].clone()\n",
    "    else:\n",
    "        lmn_coes = in_img.new_tensor([[0.2224, 0.7169, 0.0606]])\n",
    "        out_img = in_img.clone()\n",
    "\n",
    "    out_img = out_img.view((bch, 3, -1))\n",
    "    out_img = t.matmul(lmn_coes, out_img)\n",
    "    out_img = out_img.view((bch, 1, hei, wid))\n",
    "    out_img = t.clamp(out_img, 0.0, 1.0)\n",
    "\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayer CFA Demosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def demosaic(in_raw, ptn='RGGB'):\n",
    "    img_list = []\n",
    "    for index in range(in_raw.size(0)):\n",
    "        packed_raw = in_raw[index, :, :, :]\n",
    "\n",
    "        # unfold to flat bayer\n",
    "        flat_raw = np.zeros((int(in_raw.size(2) * 2), int(in_raw.size(3) * 2)))\n",
    "        flat_raw[0::2, 0::2] = packed_raw[0, :, :].clone().cpu().numpy()\n",
    "        flat_raw[0::2, 1::2] = packed_raw[1, :, :].clone().cpu().numpy()\n",
    "        flat_raw[1::2, 0::2] = packed_raw[2, :, :].clone().cpu().numpy()\n",
    "        flat_raw[1::2, 1::2] = packed_raw[3, :, :].clone().cpu().numpy()\n",
    "\n",
    "        # demosaic\n",
    "        lin_img = DDFAPD(flat_raw, pattern=ptn)\n",
    "        lin_img = in_raw.new_tensor(lin_img).permute(2, 0, 1)\n",
    "\n",
    "        # add to results\n",
    "        img_list.append(lin_img)\n",
    "\n",
    "    # stack as one tensor\n",
    "    out_img = t.stack(img_list, dim=0)\n",
    "\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illumination adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     14
    ]
   },
   "outputs": [],
   "source": [
    "def applyIlmCoes(in_img, coes, cam2xyz=None):\n",
    "    out_img = in_img.clone()\n",
    "\n",
    "    # interative enhancement\n",
    "    for index in range(coes.size(1)):\n",
    "        coe_slice = coes[:, index, :, :].unsqueeze(1)\n",
    "        lmn_img = rgb2lumin(out_img, cam2xyz)\n",
    "        res = coe_slice * (1.0 - lmn_img) * out_img\n",
    "        out_img = out_img + res\n",
    "        out_img = t.clamp(out_img, 0.0, 1.0)\n",
    "\n",
    "    return out_img\n",
    "\n",
    "\n",
    "def applyIlmDen(in_img, noise_map, denoiser, coes, cam2xyz):\n",
    "    out_img = in_img.clone()\n",
    "\n",
    "    # interative enhancement with denoising\n",
    "    for index in range(coes.size(1)):\n",
    "        coe_slice = coes[:, index, :, :].unsqueeze(1)\n",
    "        lmn_img = rgb2lumin(out_img, cam2xyz)\n",
    "        res = coe_slice * (1.0 - lmn_img) * out_img\n",
    "        out_img = denoiser(in_img, noise_map, res)\n",
    "\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def applyClrCoes(in_img, coes, cam2xyz=None):\n",
    "    pcc = [\n",
    "        '1',\\\n",
    "        'r', 'g', 'b',\\\n",
    "        'rr', 'gg', 'bb', 'rg', 'gb', 'rb',\\\n",
    "        'rrr', 'ggg', 'bbb', 'rgg', 'gbb', 'rbb', 'grr', 'bgg', 'brr', 'rgb'\n",
    "    ]\n",
    "\n",
    "    # perform demosaicing if needed\n",
    "    if in_img.size(1) == 4:\n",
    "        srgb_img = demosaic(in_img)\n",
    "    else:\n",
    "        srgb_img = in_img.clone()\n",
    "\n",
    "    # convert to sRGB color space if needed\n",
    "    if cam2xyz is not None:\n",
    "        srgb_img = cam2sRGB(srgb_img, cam2xyz)\n",
    "\n",
    "    # polynominal transforms\n",
    "    pcc_len = len(pcc)\n",
    "    out_img = t.zeros_like(srgb_img)\n",
    "    coes = coes.view(coes.size(0), -1, 1, 1)\n",
    "    for (index, poly) in enumerate(pcc):\n",
    "        poly_img = t.ones_like(out_img[:, 0, :, :])\n",
    "        for char in poly:\n",
    "            if char == '1':\n",
    "                pass\n",
    "            elif char == 'r':\n",
    "                poly_img = poly_img * srgb_img[:, 0, :, :]\n",
    "            elif char == 'g':\n",
    "                poly_img = poly_img * srgb_img[:, 1, :, :]\n",
    "            elif char == 'b':\n",
    "                poly_img = poly_img * srgb_img[:, 2, :, :]\n",
    "            else:\n",
    "                sys.exit('Unrecognized polynominal term: %s' % char)\n",
    "\n",
    "        # R\n",
    "        cur_chn = out_img[:, 0, :, :]\n",
    "        cur_coes = coes[:, index, :, :]\n",
    "        out_img[:, 0, :, :] = cur_chn + cur_coes * poly_img\n",
    "\n",
    "        # G\n",
    "        cur_chn = out_img[:, 1, :, :]\n",
    "        cur_coes = coes[:, index + pcc_len, :, :]\n",
    "        out_img[:, 1, :, :] = cur_chn + cur_coes * poly_img\n",
    "\n",
    "        # B\n",
    "        cur_chn = out_img[:, 2, :, :]\n",
    "        cur_coes = coes[:, index + 2 * pcc_len, :, :]\n",
    "        out_img[:, 2, :, :] = cur_chn + cur_coes * poly_img\n",
    "\n",
    "    out_img = t.clamp(out_img, 0.0, 1.0)\n",
    "\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     45,
     65
    ]
   },
   "outputs": [],
   "source": [
    "class vgg16Loss(t.nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(vgg16Loss, self).__init__()\n",
    "\n",
    "        # VGG input normalization\n",
    "        self.mean = t.tensor([0.485, 0.456, 0.406], device=device)\n",
    "        self.std = t.tensor([0.229, 0.224, 0.225], device=device)\n",
    "        self.mean = self.mean.view(1, -1, 1, 1)\n",
    "        self.std = self.std.view(1, -1, 1, 1)\n",
    "\n",
    "        # pretrained weights\n",
    "        features = list(tv.models.vgg16(pretrained=True).features)[:30]\n",
    "        self.features = t.nn.ModuleList(features).to(device).eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # instance normalization\n",
    "        self.insNorm = {\n",
    "            3: t.nn.InstanceNorm2d(64, affine=False),\n",
    "            8: t.nn.InstanceNorm2d(128, affine=False),\n",
    "            15: t.nn.InstanceNorm2d(256, affine=False),\n",
    "            22: t.nn.InstanceNorm2d(512, affine=False),\n",
    "            29: t.nn.InstanceNorm2d(512, affine=False)\n",
    "        }\n",
    "\n",
    "        # indices of layers to be extracted\n",
    "        self.layer_list = [3, 8, 15, 22, 29]\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        x = (img1 - self.mean.expand_as(img1)) / self.std.expand_as(img1)\n",
    "        y = (img2 - self.mean.expand_as(img2)) / self.std.expand_as(img2)\n",
    "        vgg_loss = 0.0\n",
    "\n",
    "        # compute VGG perceptual loss\n",
    "        for index, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "            if index in self.layer_list:\n",
    "                vgg_loss += mse_loss(self.insNorm[index](x),\n",
    "                                     self.insNorm[index](y))\n",
    "        vgg_loss = vgg_loss / len(self.layer_list)\n",
    "\n",
    "        return vgg_loss\n",
    "\n",
    "\n",
    "class expLoss(t.nn.Module):\n",
    "    def __init__(self, shadow=0.1, sigma=0.2):\n",
    "        super(expLoss, self).__init__()\n",
    "        self.shadow = shadow\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, in_img, out_img):\n",
    "        out_avg = t.mean(out_img, dim=1)\n",
    "        exp_loss = 1.0 - t.exp(-0.5 * (out_avg - 0.5)**2 / self.sigma**2)\n",
    "        if in_img is not None:\n",
    "            in_avg = t.mean(in_img, dim=1)\n",
    "            blk_loss = t.nn.functional.l1_loss(out_img, in_img)\n",
    "            out_loss = t.mean(t.where(in_avg < self.shadow, blk_loss,\n",
    "                                      exp_loss))\n",
    "        else:\n",
    "            out_loss = t.mean(exp_loss)\n",
    "\n",
    "        return out_loss\n",
    "\n",
    "\n",
    "class tvLoss(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tvLoss, self).__init__()\n",
    "\n",
    "    def forward(self, in_feats):\n",
    "        # compute gradients\n",
    "        pixel_dif1 = in_feats[:, :, 1:, :-1] - in_feats[:, :, :-1, :-1]\n",
    "        pixel_dif2 = in_feats[:, :, :-1, 1:] - in_feats[:, :, :-1, :-1]\n",
    "\n",
    "        # apply weighting\n",
    "        tv_loss = t.mean(t.abs(pixel_dif1) + t.abs(pixel_dif2))\n",
    "\n",
    "        return tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def applyRGBCoes(in_img, coes, cam2xyz=None):\n",
    "    out_img = in_img.clone()\n",
    "\n",
    "    # interative enhancement\n",
    "    for index in range(coes.size(1)):\n",
    "        coe_slice = coes[:, index, :, :].unsqueeze(1)\n",
    "        res = coe_slice * (1.0 - out_img) * out_img\n",
    "        out_img = out_img + res\n",
    "        out_img = t.clamp(out_img, 0.0, 1.0)\n",
    "\n",
    "    return out_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
