{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# mass includes\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import rawpy as rp\n",
    "import torch as t\n",
    "from torch.utils import data\n",
    "from shutil import rmtree\n",
    "from ipynb.fs.full.util import unifyBayerPtn, batchCrop, randHorFlip, randVerFlip\n",
    "from ipynb.fs.full.ISPColor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PairedSet(data.Dataset):\n",
    "    def __init__(self, opt):\n",
    "        self.data_root = opt.data_root\n",
    "        self.crop_size = opt.crop_size\n",
    "\n",
    "        # find all paired samples\n",
    "        self.file_list = [\n",
    "            file for file in os.listdir(os.path.join(self.data_root, 'paired'))\n",
    "            if '.pkl' in file\n",
    "        ]\n",
    "        self.file_list.sort()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load a new sample\n",
    "        file_path = os.path.join(self.data_root, 'paired',\n",
    "                                 self.file_list[index])\n",
    "        with open(file_path, 'rb') as pkl:\n",
    "            sample = pickle.load(pkl)\n",
    "        in_img = sample['zeroed']\n",
    "        out_img = sample['expert']\n",
    "\n",
    "        # random operations\n",
    "        in_img, out_img = batchCrop([in_img, out_img], self.crop_size)\n",
    "        in_img, out_img = randHorFlip([in_img, out_img])\n",
    "        in_img, out_img = randVerFlip([in_img, out_img])\n",
    "\n",
    "        # normalize\n",
    "        in_img = np.float32(in_img) / 65535.0\n",
    "        out_img = np.float32(out_img) / 65535.0\n",
    "\n",
    "        # convert to tensor\n",
    "        in_img = t.tensor(in_img).permute(2, 0, 1)\n",
    "        out_img = t.tensor(out_img).permute(2, 0, 1)\n",
    "\n",
    "        # convert to linear sRGB\n",
    "        in_img = t.where(in_img <= 0.04045, in_img / 12.92,\n",
    "                         ((in_img + 0.055) / 1.055)**2.4)\n",
    "\n",
    "        return in_img, out_img\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpaired set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class UnpairedSet(data.Dataset):\n",
    "    def __init__(self, opt):\n",
    "        self.data_root = opt.data_root\n",
    "        self.crop_size = opt.crop_size\n",
    "\n",
    "        # find all training samples\n",
    "        self.file_list = []\n",
    "        self.weight_list = []\n",
    "        with open(os.path.join(self.data_root, 'unpaired', 'prob.txt'),\n",
    "                  'r') as txt:\n",
    "            lines = txt.readlines()\n",
    "        for line in lines:\n",
    "            file, _, weight, _ = line.split(' ')\n",
    "            self.file_list.append(file + '.pkl')\n",
    "            self.weight_list.append(float(weight))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load a new sample\n",
    "        file_path = os.path.join(self.data_root, 'unpaired',\n",
    "                                 self.file_list[index])\n",
    "        with open(file_path, 'rb') as pkl:\n",
    "            sample = pickle.load(pkl)\n",
    "        lin_img = sample['lin_img']\n",
    "\n",
    "        # random operations\n",
    "        lin_img = batchCrop(lin_img, self.crop_size)\n",
    "        lin_img = randHorFlip(lin_img)\n",
    "        lin_img = randVerFlip(lin_img)\n",
    "\n",
    "        # convert to tensor\n",
    "        lin_img = t.tensor(lin_img).permute(2, 0, 1)\n",
    "\n",
    "        # generate non-linear sRGB\n",
    "        nlin_img = t.where(lin_img <= 0.0031308, 12.92 * lin_img,\n",
    "                           1.055 * (lin_img**(1 / 2.4)) - 0.055)\n",
    "\n",
    "        return lin_img, nlin_img\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DenoiseSet(data.Dataset):\n",
    "    def __init__(self, opt, cam_model, mode):\n",
    "        self.data_path = os.path.join(opt.data_root, 'denoise', cam_model,\n",
    "                                      mode)\n",
    "        self.crop_size = opt.crop_size\n",
    "        self.mode = mode\n",
    "\n",
    "        self.file_list = [\n",
    "            file for file in os.listdir(self.data_path) if '.pkl' in file\n",
    "        ]\n",
    "        self.file_list.sort()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load a new sample\n",
    "        file_path = os.path.join(self.data_path, self.file_list[index])\n",
    "        with open(file_path, 'rb') as pkl:\n",
    "            sample = pickle.load(pkl)\n",
    "        noisy_raw = sample['noisy_raw']\n",
    "        clean_raw = sample['clean_raw']\n",
    "        noise_map = sample['variance']\n",
    "        ilm_coes = sample['ilm_coes']\n",
    "        cam2xyz = sample['cam2xyz']\n",
    "\n",
    "        # random cropping\n",
    "        if self.mode == 'train':\n",
    "            noisy_raw, clean_raw, noise_map, ilm_coes = batchCrop(\n",
    "                [noisy_raw, clean_raw, noise_map, ilm_coes], self.crop_size)\n",
    "        else:\n",
    "            noisy_raw, clean_raw, noise_map, ilm_coes = batchCrop(\n",
    "                [noisy_raw, clean_raw, noise_map, ilm_coes],\n",
    "                self.crop_size,\n",
    "                centred=True)\n",
    "\n",
    "        # normalize and convert to tensor\n",
    "        noisy_raw = t.tensor(noisy_raw).permute(2, 0, 1)\n",
    "        clean_raw = t.tensor(clean_raw).permute(2, 0, 1)\n",
    "        noise_map = t.tensor(noise_map).permute(2, 0, 1)\n",
    "        ilm_coes = t.tensor(ilm_coes).permute(2, 0, 1)\n",
    "        cam2xyz = t.tensor(cam2xyz)\n",
    "\n",
    "        return noisy_raw, clean_raw, noise_map, ilm_coes, cam2xyz\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ValSet(data.Dataset):\n",
    "    def __init__(self, opt, folder):\n",
    "        self.data_path = os.path.join(opt.data_root, folder)\n",
    "\n",
    "        # find all training samples\n",
    "        self.file_list = [\n",
    "            file for file in os.listdir(os.path.join(self.data_path))\n",
    "            if '.dng' in file or '.DNG' in file\n",
    "        ]\n",
    "        self.file_list.sort()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load a new sample\n",
    "        file = self.file_list[index]\n",
    "        file_path = os.path.join(self.data_path, file)\n",
    "        with rp.imread(file_path) as raw_obj:\n",
    "            cfa_data = raw_obj.raw_image_visible.copy()\n",
    "            cfa_mask = raw_obj.raw_colors_visible\n",
    "            blk_level = raw_obj.black_level_per_channel\n",
    "            sat_level = raw_obj.white_level\n",
    "            cfa_type = raw_obj.raw_pattern\n",
    "\n",
    "        # normalize to 0-1\n",
    "        cfa_data = cfa_data.astype(np.float64)\n",
    "        cfa_data[cfa_mask == 0] = cfa_data[cfa_mask == 0] - blk_level[0]\n",
    "        cfa_data[cfa_mask == 1] = cfa_data[cfa_mask == 1] - blk_level[1]\n",
    "        cfa_data[cfa_mask == 2] = cfa_data[cfa_mask == 2] - blk_level[2]\n",
    "        cfa_data[cfa_mask == 3] = cfa_data[cfa_mask == 3] - blk_level[3]\n",
    "        cfa_data = cfa_data / (sat_level - max(blk_level))\n",
    "        cfa_data = np.clip(cfa_data, 0.0, 1.0)\n",
    "\n",
    "        # Bayer pattern unification\n",
    "        cfa_data = unifyBayerPtn(cfa_data, cfa_type)\n",
    "\n",
    "        # pack to 4-channel raw\n",
    "        noisy_raw = np.zeros((math.ceil(cfa_data.shape[0] / 2),\n",
    "                              math.ceil(cfa_data.shape[1] / 2), 4))\n",
    "        noisy_raw[:, :, 0] = cfa_data[0::2, 0::2]\n",
    "        noisy_raw[:, :, 1] = cfa_data[0::2, 1::2]\n",
    "        noisy_raw[:, :, 2] = cfa_data[1::2, 0::2]\n",
    "        noisy_raw[:, :, 3] = cfa_data[1::2, 1::2]\n",
    "\n",
    "        # crop to square (even number)\n",
    "        hei, wid, _ = noisy_raw.shape\n",
    "        hei = hei // 32 * 32  # to avoid odd number pooling\n",
    "        wid = wid // 32 * 32  # to avoid odd number pooling\n",
    "        offset = int(abs(wid - hei) / 2)\n",
    "        if hei < wid:\n",
    "            noisy_raw = noisy_raw[:hei, offset:offset + hei, :]\n",
    "        elif hei > wid:\n",
    "            noisy_raw = noisy_raw[offset:offset + wid, :wid, :]\n",
    "\n",
    "        # compute color transform matrix\n",
    "        metadata = extMetadata(file_path)\n",
    "        wp_xyz, interp_w = cam2xyzWP(metadata)\n",
    "        cam2xyz = cam2xyzD50(metadata, wp_xyz, interp_w)\n",
    "\n",
    "        # convert to tensor\n",
    "        noisy_raw = t.tensor(noisy_raw.astype(np.float32)).permute(2, 0, 1)\n",
    "        cam2xyz = t.tensor(cam2xyz.astype(np.float32))\n",
    "\n",
    "        return noisy_raw, cam2xyz, file\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TestSet(data.Dataset):\n",
    "    def __init__(self, data_root, folder):\n",
    "        self.data_path = os.path.join(data_root, folder, 'dng')\n",
    "\n",
    "        # load noise models\n",
    "        model_path = os.path.join(data_root, folder, 'noiseModel')\n",
    "        with open(os.path.join(model_path, 'model_params.pkl'), 'rb') as pkl:\n",
    "            self.noise_params = pickle.load(pkl)\n",
    "\n",
    "        # find all training samples\n",
    "        self.file_list = [\n",
    "            file for file in os.listdir(os.path.join(self.data_path))\n",
    "            if '.dng' in file or '.DNG' in file\n",
    "        ]\n",
    "        self.file_list.sort()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load a new sample\n",
    "        file = self.file_list[index]\n",
    "        file_path = os.path.join(self.data_path, file)\n",
    "        with rp.imread(file_path) as raw_obj:\n",
    "            cfa_data = raw_obj.raw_image_visible.copy()\n",
    "            cfa_mask = raw_obj.raw_colors_visible\n",
    "            blk_level = raw_obj.black_level_per_channel\n",
    "            sat_level = raw_obj.white_level\n",
    "            cfa_type = raw_obj.raw_pattern\n",
    "\n",
    "        # normalize to 0-1\n",
    "        cfa_data = cfa_data.astype(np.float64)\n",
    "        cfa_data[cfa_mask == 0] = cfa_data[cfa_mask == 0] - blk_level[0]\n",
    "        cfa_data[cfa_mask == 1] = cfa_data[cfa_mask == 1] - blk_level[1]\n",
    "        cfa_data[cfa_mask == 2] = cfa_data[cfa_mask == 2] - blk_level[2]\n",
    "        cfa_data[cfa_mask == 3] = cfa_data[cfa_mask == 3] - blk_level[3]\n",
    "        cfa_data = cfa_data / (sat_level - max(blk_level))\n",
    "        cfa_data = np.clip(cfa_data, 0.0, 1.0)\n",
    "\n",
    "        # Bayer pattern unification\n",
    "        cfa_data = unifyBayerPtn(cfa_data, cfa_type)\n",
    "\n",
    "        # pack to 4-channel raw\n",
    "        noisy_raw = np.zeros((math.ceil(cfa_data.shape[0] / 2),\n",
    "                              math.ceil(cfa_data.shape[1] / 2), 4))\n",
    "        noisy_raw[:, :, 0] = cfa_data[0::2, 0::2]\n",
    "        noisy_raw[:, :, 1] = cfa_data[0::2, 1::2]\n",
    "        noisy_raw[:, :, 2] = cfa_data[1::2, 0::2]\n",
    "        noisy_raw[:, :, 3] = cfa_data[1::2, 1::2]\n",
    "\n",
    "        # crop to square (even number)\n",
    "        hei, wid, _ = noisy_raw.shape\n",
    "        hei = hei // 32 * 32  # to avoid odd number pooling\n",
    "        wid = wid // 32 * 32  # to avoid odd number pooling\n",
    "        offset = int(abs(wid - hei) / 2)\n",
    "        if hei < wid:\n",
    "            noisy_raw = noisy_raw[:hei, offset:offset + hei, :]\n",
    "        elif hei > wid:\n",
    "            noisy_raw = noisy_raw[offset:offset + wid, :wid, :]\n",
    "\n",
    "        # compute ISO, noise model, and color matrix\n",
    "        metadata = extMetadata(file_path)\n",
    "        cam_iso = metadata['iso']\n",
    "        noise_model = self.noise_params[cam_iso]\n",
    "        wp_xyz, interp_w = cam2xyzWP(metadata)\n",
    "        cam2xyz = cam2xyzD50(metadata, wp_xyz, interp_w)\n",
    "\n",
    "        # compute noise map\n",
    "        noise_map = noise_model['k'] * noisy_raw + noise_model[\n",
    "            'sig_r']**2 + noise_model[\n",
    "                'sig_tl']**2 + noise_model['hf_step']**2 / 3\n",
    "\n",
    "        # convert to tensor\n",
    "        noisy_raw = t.tensor(noisy_raw.astype(np.float32)).permute(2, 0, 1)\n",
    "        noise_map = t.tensor(noise_map.astype(np.float32)).permute(2, 0, 1)\n",
    "        cam2xyz = t.tensor(cam2xyz.astype(np.float32))\n",
    "\n",
    "        return noisy_raw, noise_map, cam2xyz, file\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.file_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
